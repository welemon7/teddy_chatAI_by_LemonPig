import streamlit as st
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
import torch
import time
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain_community.llms import HuggingFacePipeline
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.document_loaders import TextLoader
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from sentence_transformers import CrossEncoder
import os

##########################################  streamlit run test.py å‰ç«¯ç•Œé¢
# é¡µé¢æ ·å¼é…ç½®
st.set_page_config(
    page_title="LemonAI æ™ºèƒ½åŠ©æ‰‹",
    page_icon="ğŸ‹",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è‡ªå®šä¹‰ CSS æ ·å¼
st.markdown("""
<style>
    [data-testid="stAppViewContainer"] {
        background: linear-gradient(135deg, #FDFFBC 0%, #FFEAC8 100%);
    }
    .stChatMessage {
        padding: 15px 20px;
        border-radius: 18px !important;
        margin: 12px 0;
        box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    .user .stChatMessage {
        background: #F8FFD0;
        border: 1px solid #E0E9A3;
    }
    .assistant .stChatMessage {
        background: #FFF9E6;
        border: 1px solid #F5E3A9;
    }
    .lemon-header {
        font-family: 'Microsoft YaHei';
        color: #5E8B2C;
        text-shadow: 1px 1px 2px rgba(0,0,0,0.1);
    }
    .fas {
        font-family: 'Font Awesome 5 Free' !important;
    }
    @import url('https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css');
</style>
""", unsafe_allow_html=True)

# ä¾§è¾¹æ é…ç½®
with st.sidebar:
    # LOGO
    st.image("https://cdn-icons-png.flaticon.com/512/3199/3199018.png", width=80)

    # å¯¹è¯è®¾ç½®
    st.divider()
    st.markdown("**ğŸ‹ å¯¹è¯è®¾ç½®  ğŸ–**")
    temperature = st.slider("åˆ›æ„æ€è€ƒèƒ½åŠ›", 0.0, 1.0, 0.3,
                            help="åˆ›é€ åº¦è¶Šé«˜ï¼Œè¶Šé€‚åˆå¼€æ”¾æ€§çš„é—®é¢˜")

    # çŸ¥è¯†åº“è®¾ç½®
    st.divider()
    st.markdown("**ğŸŒ³ çŸ¥è¯†åº“è®¾ç½®**")
    search_depth = st.slider("æ£€ç´¢æ·±åº¦", 1, 18, 3,
                             help="è°ƒæ•´å‚è€ƒæ–‡æ¡£çš„æ•°é‡")
    confidence_threshold = st.slider("ç½®ä¿¡é˜ˆå€¼", 0.5, 1.0, 0.7,
                                     help="è®¾ç½®çŸ¥è¯†åŒ¹é…çš„ç½®ä¿¡åº¦è¦æ±‚")

    # çŸ¥è¯†åº“æ›´æ–°åŠŸèƒ½
    st.divider()
    st.markdown("**ğŸ“š çŸ¥è¯†åº“æ›´æ–°**")
    uploaded_file = st.file_uploader("ä¸Šä¼ æ–°æ–‡æ¡£ï¼ˆæ”¯æŒ TXT/PDFï¼‰", type=["txt", "pdf"])
    if st.button("æ›´æ–°çŸ¥è¯†åº“", type="primary"):
        if uploaded_file is not None:
            with st.spinner("æ­£åœ¨æ›´æ–°çŸ¥è¯†åº“..."):
                try:
                    st.session_state.qa_system.update_knowledge_base(uploaded_file)
                    st.success("çŸ¥è¯†åº“æ›´æ–°æˆåŠŸï¼")
                except Exception as e:
                    st.error(f"çŸ¥è¯†åº“æ›´æ–°å¤±è´¥: {str(e)}")
        else:
            st.warning("è¯·ä¸Šä¼ ä¸€ä¸ª TXT æˆ– PDF æ–‡ä»¶ã€‚")

    st.divider()
    st.markdown("**ğŸ”„ çŸ¥è¯†åº“é‡ç½®**")
    if st.button("é‡ç½®åˆ°åˆå§‹çŸ¥è¯†åº“", type="primary", help="å°†çŸ¥è¯†åº“æ¢å¤åˆ°åŸå§‹æ¯”èµ›æ–‡æ¡£çŠ¶æ€"):
        with st.spinner("æ­£åœ¨æ¢å¤åˆå§‹çŸ¥è¯†åº“..."):
            try:
                import shutil
                import os

                # è·¯å¾„é…ç½®
                FAISS_INDEX_PATH = "./faiss_index_data"
                INITIAL_BACKUP_PATH = "./initial_faiss_backup"

                # åˆ é™¤å½“å‰ç´¢å¼•
                if os.path.exists(FAISS_INDEX_PATH):
                    shutil.rmtree(FAISS_INDEX_PATH)

                # ä»å¤‡ä»½æ¢å¤åˆå§‹ç´¢å¼•
                shutil.copytree(INITIAL_BACKUP_PATH, FAISS_INDEX_PATH)

                # é‡æ–°åˆå§‹åŒ–ç³»ç»Ÿ
                st.session_state.qa_system.initialize_components()
                st.toast("ğŸ‹ çŸ¥è¯†åº“å·²æ¢å¤åˆ°åˆå§‹çŠ¶æ€!", icon="âœ…")
            except Exception as e:
                st.error(f"æ¢å¤å¤±è´¥: {str(e)}")

    if st.button("æ¸…ç©ºå¯¹è¯å†å²", type="primary"):
        st.session_state.messages = [
            {"role": "assistant", "content": "hi~æˆ‘æ˜¯LemonAIï¼Œæ”¯æŒæ¯”èµ›ä¿¡æ¯çš„çŸ¥è¯†åº“é—®ç­”ï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®æ‚¨å‘¢ï¼Ÿ"}
        ]
        st.rerun()

# é¡¶éƒ¨æ ‡é¢˜æ 
header_col1, header_col2, header_col3 = st.columns([0.15, 0.7, 0.15])
with header_col1:
    st.image("https://cdn-icons-png.flaticon.com/512/3199/3199018.png", width=80)
with header_col2:
    st.markdown('<h1 class="lemon-header">ğŸ‹ğŸ– LemonPigAI æ¯”èµ›ç­”ç–‘æ™ºèƒ½åŠ©æ‰‹</h1>', unsafe_allow_html=True)
with header_col3:
    st.markdown("""
    <div style="text-align: right; margin-top: 15px;">
        <span style="font-size: 0.9em; color: #888; font-style: italic;">
            <i class="fas fa-lightbulb" style="color: #FFD700;"></i>
            ğŸ‹ å¦‚æœå›ç­”ä¸æ»¡æ„æˆ–ä¸ç¡®å®šå»ºè®®æ‚¨å†æé—®ä¸€æ¬¡ï¼Œæˆ‘ä¼šæ ¹æ®ä¸Šæ–‡å†æ¬¡åšå‡ºä¼˜åŒ–å“¦! ğŸ– 
        </span>
    </div>
    """, unsafe_allow_html=True)

# æ¶ˆæ¯å†å²å¤„ç†
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "assistant", "content": "hi~æˆ‘æ˜¯LemonAIï¼Œæ”¯æŒæ¯”èµ›ä¿¡æ¯çš„çŸ¥è¯†åº“é—®ç­”ï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®æ‚¨å‘¢ï¼Ÿ"}
    ]

# é…ç½®å‚æ•°
QWEN_MODEL_NAME = "./LLM_small"
EMBEDDING_MODEL_PATH = "./vector_model"
FAISS_INDEX_PATH = "./faiss_index_data"


# æç¤ºæ¨¡æ¿
def load_enhanced_prompt():
    return PromptTemplate(
        template="""### å‚è€ƒå†…å®¹ï¼š
{context}
### ç”¨æˆ·é—®é¢˜ï¼š
{question}
### å›ç­”è¦æ±‚ï¼š
1. åŸºäºå‚è€ƒå†…å®¹æ€»ç»“ï¼Œç¦æ­¢å¤åˆ¶åŸæ–‡
2. åˆ†ç‚¹åˆ—å‡ºæ ¸å¿ƒä¿¡æ¯ï¼ˆ1. 2. 3.ï¼‰
3. æ¯ä¸ªè¦ç‚¹<30å­—ï¼Œæ€»å›ç­”<100å­—
4. æ ‡æ³¨å¼•ç”¨æ¥æº[æ•°å­—]
5. æ— ç›¸å…³ä¿¡æ¯æ—¶è¯´æ˜
### ä¸“ä¸šå›ç­”ï¼š""",
        input_variables=["context", "question"]
    )



############################################################    çŸ¥è¯†åº“æ£€ç´¢åŒ¹é…éƒ¨åˆ†
# æ™ºèƒ½é—®ç­”ç³»ç»Ÿ
class EnhancedQASystem:
    def __init__(self):
        self.vector_store = None
        self.qa_chain = None
        self.embeddings = None
        self.create_initial_backup()
        self.cross_encoder = None
        self.load_cross_encoder()  # åˆå§‹åŒ–æ—¶åŠ è½½


    def load_cross_encoder(self):
        # åŠ è½½ä¸­æ–‡ä¼˜åŒ–çš„Cross-Encoderæ¨¡å‹ï¼ˆæ”¯æŒæŸ¥è¯¢-æ–‡æ¡£å¯¹è¯„åˆ†ï¼‰
        self.cross_encoder = CrossEncoder(
            "./cross_encode_model",
            device="cuda" if torch.cuda.is_available() else "cpu"
        )

    def initialize_components(self):
        # åŠ è½½åµŒå…¥æ¨¡å‹
        self.embeddings = HuggingFaceEmbeddings(
            model_name=EMBEDDING_MODEL_PATH,
            model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'}
        )

        # åŠ è½½å‘é‡åº“
        self.vector_store = FAISS.load_local(
            FAISS_INDEX_PATH,
            self.embeddings,
            allow_dangerous_deserialization=True
        )
        self.retriever = self.vector_store.as_retriever(
            search_kwargs={"k": 18}  # æ‰©å¤§åŸºç¡€æ£€ç´¢èŒƒå›´ï¼Œåç»­é‡æ’åº
        )

        # åˆå§‹åŒ–è¯­è¨€æ¨¡å‹
        llm = self.load_qwen_model()

        # æ„å»º QA é“¾
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=self.vector_store.as_retriever(search_kwargs={"k": search_depth}),
            chain_type_kwargs={"prompt": load_enhanced_prompt()},
            return_source_documents=True
        )

    # æ£€ç´¢é€»è¾‘ï¼ŒåŠ å…¥Cross-Encoderé‡æ’åº
    def get_relevant_documents(self, query):
        # åŸºç¡€å‘é‡æ£€ç´¢
        base_docs = self.retriever.get_relevant_documents(query)

        if not base_docs:
            return []

        # ç”ŸæˆæŸ¥è¯¢-æ–‡æ¡£å¯¹
        input_pairs = [(query, doc.page_content) for doc in base_docs]

        # Cross-Encoderè¯„åˆ†ï¼ˆè¿”å›æ¦‚ç‡åˆ†æ•°ï¼Œå€¼è¶Šé«˜ç›¸å…³æ€§è¶Šå¼ºï¼‰
        scores = self.cross_encoder.predict(input_pairs)

        # åˆå¹¶æ–‡æ¡£å’Œåˆ†æ•°å¹¶æŒ‰åˆ†æ•°é™åºæ’åº
        scored_docs = sorted(zip(base_docs, scores), key=lambda x: -x[1])

        # åº”ç”¨ç½®ä¿¡åº¦è¿‡æ»¤å’Œæ·±åº¦é™åˆ¶
        filtered_docs = [doc for doc, score in scored_docs
                         if score >= confidence_threshold  # ä½¿ç”¨ä¾§è¾¹æ é…ç½®çš„é˜ˆå€¼
                         ][:search_depth]  # ä½¿ç”¨ä¾§è¾¹æ é…ç½®çš„æ£€ç´¢æ·±åº¦

        return filtered_docs

    # QAé“¾æ„å»ºé€»è¾‘ï¼ˆä½¿ç”¨è‡ªå®šä¹‰æ£€ç´¢å‡½æ•°ï¼‰
    def get_qa_chain(self):
        llm = self.load_qwen_model()
        return RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=self.retriever,  # ä¿ç•™åŸºç¡€Retriever
            chain_type_kwargs={"prompt": load_enhanced_prompt()},
            return_source_documents=True
        )



    def load_qwen_model(self):
        tokenizer = AutoTokenizer.from_pretrained(QWEN_MODEL_NAME, trust_remote_code=True)
        model = AutoModelForCausalLM.from_pretrained(
            QWEN_MODEL_NAME,
            torch_dtype=torch.float32,
            device_map="auto",
            trust_remote_code=True
        )

        llm_pipeline = pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            max_new_tokens=500,
            temperature=0.4,
            top_p=0.85,
            repetition_penalty=1.2,
            device_map="auto"
        )
        return HuggingFacePipeline(pipeline=llm_pipeline)


    # psï¼šè¿™é‡Œæˆ‘ä»¬æ›¿æ¢åŸä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•ï¼ˆä½¿ç”¨Cross-Encoderåˆ†æ•°ï¼‰
    def get_similarity_scores(self, query, documents):
        input_pairs = [(query, doc.page_content) for doc in documents]
        return self.cross_encoder.predict(input_pairs)

    # æ›´æ–°çŸ¥è¯†åº“çš„æ–¹æ³•
    def update_knowledge_base(self, uploaded_file):
        # åˆ›å»ºä¸´æ—¶ç›®å½•ä¿å­˜ä¸Šä¼ æ–‡ä»¶
        import tempfile
        temp_dir = tempfile.mkdtemp()
        file_path = os.path.join(temp_dir, uploaded_file.name)

        # ä¿å­˜ä¸Šä¼ æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
        with open(file_path, "wb") as f:
            f.write(uploaded_file.getbuffer())

        # åˆ¤æ–­æ–‡ä»¶ç±»å‹å¹¶é€‰æ‹©åŠ è½½å™¨
        if file_path.lower().endswith('.pdf'):
            loader = PyPDFLoader(file_path)
        elif file_path.lower().endswith(('.txt', '.text')):
            loader = TextLoader(file_path)
        else:
            raise ValueError("Unsupported file type")

        # åŠ è½½å’Œå¤„ç†æ–‡æ¡£
        documents = loader.load()
        text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
        docs = text_splitter.split_documents(documents)

        # æ›´æ–°å‘é‡åº“
        self.vector_store.add_documents(docs)
        self.vector_store.save_local(FAISS_INDEX_PATH)

        # é‡æ–°æ„å»º QA é“¾
        llm = self.load_qwen_model()
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=self.vector_store.as_retriever(search_kwargs={"k": search_depth}),
            chain_type_kwargs={"prompt": load_enhanced_prompt()},
            return_source_documents=True
        )

        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        try:
            shutil.rmtree(temp_dir)
        except Exception as e:
            print(f"Error cleaning temp files: {str(e)}")

    def create_initial_backup(self):
        #åˆ›å»ºåˆå§‹çŸ¥è¯†åº“å¤‡ä»½
        FAISS_INDEX_PATH = "./faiss_index_data"
        INITIAL_BACKUP_PATH = "./initial_faiss_backup"

        if not os.path.exists(INITIAL_BACKUP_PATH):
            try:
                shutil.copytree(FAISS_INDEX_PATH, INITIAL_BACKUP_PATH)
                print("Initial knowledge base backup created.")
            except Exception as e:
                print(f"Backup creation failed: {str(e)}")


# åˆå§‹åŒ–çŸ¥è¯†åº“ç³»ç»Ÿ
if "qa_system" not in st.session_state:
    with st.spinner("çŸ¥è¯†åº“ç³»ç»Ÿåˆå§‹åŒ–ä¸­..."):
        st.session_state.qa_system = EnhancedQASystem()
        st.session_state.qa_system.initialize_components()




################################################################    äºŒæ¬¡ç»è¿‡æ¨¡å‹ï¼Œè¿›è¡Œæ¨ç†
@st.cache_resource(show_spinner="lemonAIæ­£åœ¨å‡ºç‚‰...")
def load_model():
    model_path = "LLM_small"
    try:
        tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True
        )
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float32,
            device_map='cpu',
            low_cpu_mem_usage=True,
            trust_remote_code=True
        ).eval()
        return model, tokenizer
    except Exception as e:
        st.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
        return None, None


model, tokenizer = load_model()


# è‡ªå®šä¹‰ç”Ÿæˆå‡½æ•°ï¼ˆæ•´åˆçŸ¥è¯†åº“å›ç­”å’Œç­”æ¡ˆç®€åŒ–ï¼‰
def generate_response(prompt, history):
    # å› ä¸ºè¦è¿”å›ä¸¤ä¸ªå€¼
    lemon_keywords_1 = ["ä½ å¥½", "hi", "hello"]
    if any(keyword in prompt for keyword in lemon_keywords_1):
        return "ä½ å¥½~ ğŸ‹æˆ‘èƒ½å¸®ä½ æ›´æ¸…æ¥šçš„äº†è§£ä¸€äº›æ¯”èµ›ä¿¡æ¯ï¼Œæœ‰ä»€ä¹ˆæƒ³é—®çš„å˜›,è¯·ç›´æ¥é—®é—®é¢˜å“¦ï¼Œä¸ç”¨å†å’Œæˆ‘æ‰“æ‹›å‘¼äº†~", ""
    lemon_keywords_2 = ["æ‹œæ‹œ", "bye"]
    if any(keyword in prompt for keyword in lemon_keywords_2):
        return "æˆ‘éšæ—¶åœ¨æ ‘ä¸Šå†ç­‰ä½ æé—®~ å†ä¼š~ ğŸ‹", ""
    lemon_keywords_3 = ["æƒ…æ„Ÿ", "æ„Ÿæƒ…"]
    if any(keyword in prompt for keyword in lemon_keywords_3):
        return "ğŸ‹ ä¹Ÿå–œæ¬¢ä½ ï¼Œæ¯”èµ›åŠ æ²¹~", ""

    # é€šè¿‡çŸ¥è¯†åº“ç³»ç»Ÿç”Ÿæˆä¸“ä¸šå›ç­”
    with st.spinner("ğŸ‹ æ­£åœ¨æ£€ç´¢çŸ¥è¯†åº“..."):
        try:
            qa_system = st.session_state.qa_system
            # ä½¿ç”¨Cross-Encoderä¼˜åŒ–åçš„æ–‡æ¡£æ£€ç´¢
            relevant_docs = qa_system.get_relevant_documents(prompt)

            if not relevant_docs:
                st.toast("æœªæ‰¾åˆ°è¶³å¤Ÿç›¸å…³çš„çŸ¥è¯†åº“å†…å®¹", icon="ğŸ‹")
                return "è¿™ä¸ªé—®é¢˜åœ¨çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°è¶³å¤Ÿçš„ç›¸å…³ä¿¡æ¯å‘¢ï¼Œå¯ä»¥å°è¯•æ¢ä¸ªè§’åº¦æé—®~", ""

            similarity_scores = qa_system.get_similarity_scores(prompt, relevant_docs)
            max_similarity = max(similarity_scores)

            # æ„å»ºQAè¾“å…¥ï¼ˆåŒ…å«é‡æ’åºåçš„æ–‡æ¡£ï¼‰
            kb_result = qa_system.qa_chain({
                "query": prompt,
                "context": [doc.page_content for doc in relevant_docs]  # ç›´æ¥ä¼ å…¥è¿‡æ»¤åçš„æ–‡æ¡£
            })
            lemon_answer = kb_result["result"]

            # åœ¨å±•å¼€æ¡†æ˜¾ç¤ºCross-Encoderè¯„åˆ†ï¼ˆæ›´å‡†ç¡®çš„è¯­ä¹‰ç›¸ä¼¼åº¦ï¼‰
            with st.expander("æŸ¥çœ‹é—®é¢˜ä¸çŸ¥è¯†åº“çš„åŒ¹é…åº¦ï¼ˆCross-Encoderè¯„åˆ†ï¼‰", expanded=False):
                for i, (doc, score) in enumerate(zip(relevant_docs, similarity_scores)):
                    st.markdown(f"æ–‡æ¡£ {i+1} è¯­ä¹‰ç›¸å…³åº¦: {score:.3f}")
                    # å¯æ˜¾ç¤ºæ–‡æ¡£æ¥æºï¼ˆå¦‚æœæœ‰metadataï¼‰
                    if doc.metadata.get("source"):
                        st.markdown(f"æ¥æº: {doc.metadata['source']}")
        except Exception as e:
            lemon_answer = "çŸ¥è¯†åº“æ£€ç´¢å¤±è´¥ï¼Œè¯·é‡è¯•"

    # ç»„åˆç®€åŒ– prompt
    simplify_prompt = f"ç”¨æˆ·é—®é¢˜ï¼š{prompt}\nä¸“ä¸šå›ç­”ï¼š{lemon_answer}\nè¦æ±‚ï¼šæ ¹æ®é—®é¢˜æ‰¾åˆ°ç­”æ¡ˆå¹¶ç®€è¦æ¦‚æ‹¬ï¼Œä¸€å®šè¦ç²¾å‡†ï¼Œå¦‚æœä½ ä½ è§‰å¾—ç­”æ¡ˆä¸åŒ¹é…é—®é¢˜ï¼Œä½ å¯ä»¥è‡ªå·±å†æ ¹æ®é—®é¢˜åŠ ä¸€ç‚¹ç‚¹å¼€æ”¾æ€§çš„æ³›åŒ–ç­”æ¡ˆ"
    # å¯¹è¯æ¨¡å‹ç”Ÿæˆç®€åŒ–å›ç­”
    with st.status("ğŸ‹ æ­£åœ¨æ ‘ä¸Šç”Ÿæˆç­”æ¡ˆä¸­...", expanded=False) as status:
        time.sleep(0.5)
        messages = history + [{"role": "user", "content": simplify_prompt}]
        try:
            text = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            inputs = tokenizer(text, return_tensors="pt")

            outputs = model.generate(
                inputs.input_ids,
                max_new_tokens=500,
                temperature=temperature,
                top_p=0.9,
                repetition_penalty=1.1,
                pad_token_id=tokenizer.eos_token_id
            )

            response = tokenizer.decode(
                outputs[0][len(inputs.input_ids[0]):],
                skip_special_tokens=True
            )
            status.update(label="ğŸ‹ æ€è€ƒå®Œæˆï¼", state="complete")
            return response, lemon_answer
        except Exception as e:
            return f"æŠ¥æ„æ€ ğŸ‹ è¿˜ä¸å¤Ÿæˆç†Ÿï¼Œæ€è€ƒå¤±è´¥â€¦â€¦ ", ""



##################################################  ä¸»é¢æ¿
# ä¸»ç•Œé¢é€»è¾‘
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("è¾“å…¥æ‚¨çš„é—®é¢˜å§..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # ç”ŸæˆçŸ¥è¯†åº“å›ç­”å’Œç®€åŒ–å›ç­”
    simplified_answer, kb_answer = generate_response(prompt, st.session_state.messages[:-1])

    # æ·»åŠ å¯¹è¯å†å²
    st.session_state.messages.append({
        "role": "assistant",
        "content": simplified_answer,
        "kb_answer": kb_answer
    })

    with st.chat_message("assistant"):
        st.markdown(simplified_answer)

    # æ˜¾ç¤ºçŸ¥è¯†åº“åŸå§‹å›ç­”
    if kb_answer:
        with st.expander("æŸ¥çœ‹çŸ¥è¯†åº“è¯¦ç»†å›ç­”", expanded=False):
            st.markdown(f"**çŸ¥è¯†åº“ä¸“ä¸šå›ç­”ï¼š**\n{kb_answer}")

